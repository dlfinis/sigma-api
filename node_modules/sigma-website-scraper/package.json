{
  "_args": [
    [
      {
        "hosted": {
          "directUrl": "https://raw.githubusercontent.com/dlfinis/sigma-node-website-scraper/master/package.json",
          "gitUrl": "git://github.com/dlfinis/sigma-node-website-scraper.git",
          "httpsUrl": "git+https://github.com/dlfinis/sigma-node-website-scraper.git",
          "shortcut": "github:dlfinis/sigma-node-website-scraper",
          "ssh": "git@github.com:dlfinis/sigma-node-website-scraper.git",
          "sshUrl": "git+ssh://git@github.com/dlfinis/sigma-node-website-scraper.git",
          "type": "github"
        },
        "name": "sigma-website-scraper",
        "raw": "sigma-website-scraper@git+https://github.com/dlfinis/sigma-node-website-scraper.git",
        "rawSpec": "git+https://github.com/dlfinis/sigma-node-website-scraper.git",
        "scope": null,
        "spec": "git+https://github.com/dlfinis/sigma-node-website-scraper.git",
        "type": "hosted"
      },
      "/home/dlfinis/DevProjects/sigma/sigma-api"
    ]
  ],
  "_from": "git+https://github.com/dlfinis/sigma-node-website-scraper.git",
  "_id": "sigma-website-scraper@1.0.4",
  "_inCache": true,
  "_installable": true,
  "_location": "/sigma-website-scraper",
  "_phantomChildren": {},
  "_requested": {
    "hosted": {
      "directUrl": "https://raw.githubusercontent.com/dlfinis/sigma-node-website-scraper/master/package.json",
      "gitUrl": "git://github.com/dlfinis/sigma-node-website-scraper.git",
      "httpsUrl": "git+https://github.com/dlfinis/sigma-node-website-scraper.git",
      "shortcut": "github:dlfinis/sigma-node-website-scraper",
      "ssh": "git@github.com:dlfinis/sigma-node-website-scraper.git",
      "sshUrl": "git+ssh://git@github.com/dlfinis/sigma-node-website-scraper.git",
      "type": "github"
    },
    "name": "sigma-website-scraper",
    "raw": "sigma-website-scraper@git+https://github.com/dlfinis/sigma-node-website-scraper.git",
    "rawSpec": "git+https://github.com/dlfinis/sigma-node-website-scraper.git",
    "scope": null,
    "spec": "git+https://github.com/dlfinis/sigma-node-website-scraper.git",
    "type": "hosted"
  },
  "_requiredBy": [
    "/"
  ],
  "_resolved": "git+https://github.com/dlfinis/sigma-node-website-scraper.git#fa64f4b2bcb3572844b026e85434dd14f83c68c9",
  "_shasum": "9122c2febe5ce5a5eaee1f36fb6e3f7feaa75d97",
  "_shrinkwrap": null,
  "_spec": "sigma-website-scraper@git+https://github.com/dlfinis/sigma-node-website-scraper.git",
  "_where": "/home/dlfinis/DevProjects/sigma/sigma-api",
  "author": "",
  "bugs": {
    "url": "https://github.com/s0ph1e/node-website-scraper/issues"
  },
  "dependencies": {
    "bluebird": "^3.0.1",
    "cheerio": "0.20.0",
    "compare-urls": "^1.0.0",
    "css-url-parser": "^1.0.0",
    "fs-extra": "^0.30.0",
    "lodash": "^4.11.1",
    "request": "^2.42.0",
    "srcset": "^1.0.0"
  },
  "description": "Download website to a local directory (including all css, images, js, etc.)",
  "devDependencies": {
    "codeclimate-test-reporter": "^0.3.1",
    "coveralls": "^2.11.8",
    "eslint": "^2.8.0",
    "istanbul": "^0.4.0",
    "mocha": "^2.2.5",
    "nock": "^8.0.0",
    "proxyquire": "^1.7.3",
    "should": "^9.0.0",
    "sinon": "^1.15.4",
    "sinon-as-promised": "^4.0.0"
  },
  "gitHead": "fa64f4b2bcb3572844b026e85434dd14f83c68c9",
  "homepage": "https://github.com/s0ph1e/node-website-scraper",
  "keywords": [
    "scrape",
    "scraper",
    "download",
    "web",
    "url",
    "page",
    "site",
    "html",
    "css",
    "image",
    "js"
  ],
  "license": "MIT",
  "main": "index.js",
  "name": "sigma-website-scraper",
  "optionalDependencies": {},
  "readme": "## Introduction\nDownload website to a local directory (including all css, images, js, etc.)\n\n[![Build Status](https://img.shields.io/travis/s0ph1e/node-website-scraper/master.svg?style=flat)](https://travis-ci.org/s0ph1e/node-website-scraper)\n[![Test Coverage](https://codeclimate.com/github/s0ph1e/node-website-scraper/badges/coverage.svg)](https://codeclimate.com/github/s0ph1e/node-website-scraper/coverage)\n[![Code Climate](https://codeclimate.com/github/s0ph1e/node-website-scraper/badges/gpa.svg)](https://codeclimate.com/github/s0ph1e/node-website-scraper)\n[![Version](https://img.shields.io/npm/v/website-scraper.svg?style=flat)](https://www.npmjs.org/package/website-scraper)\n[![Downloads](https://img.shields.io/npm/dm/website-scraper.svg?style=flat)](https://www.npmjs.org/package/website-scraper)\n[![Dependency Status](https://david-dm.org/s0ph1e/node-website-scraper.svg?style=flat)](https://david-dm.org/s0ph1e/node-website-scraper)\n[![Gitter](https://badges.gitter.im/s0ph1e/node-website-scraper.svg)](https://gitter.im/s0ph1e/node-website-scraper?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\n[![NPM Stats](https://nodei.co/npm/website-scraper.png?downloadRank=true&stars=true)](https://www.npmjs.org/package/website-scraper)\n\nThis module of node.js is a fork , this have the visualization of a modal in tha action of click in each one of a elements with attrib href.\nYou can try it in [demo app](https://scraper.nepochataya.pp.ua/) ([source](https://github.com/s0ph1e/web-scraper))\n\n## Installation\n```\nnpm install website-scraper\n```\n\n## Usage\n```javascript\nvar scraper = require('website-scraper');\nvar options = {\n  urls: ['http://nodejs.org/'],\n  directory: '/path/to/save/',\n};\n\n// with callback\nscraper.scrape(options, function (error, result) {\n\t/* some code here */\n});\n\n// or with promise\nscraper.scrape(options).then(function (result) {\n\t/* some code here */\n});\n```\n\n## API\n### scrape(options, callback)\nMakes requests to `urls` and saves all files found with `sources` to `directory`.\n\n**options** - object containing next options:\n\n - `urls`: array of urls to load and filenames for them *(required, see example below)*\n - `urlFilter`: function which is called for each url to check whether it should be scraped. *(optional, see example below)*\n - `directory`: path to save loaded files *(required)*\n - `filenameGenerator`: name of one of the bundled filenameGenerators, or a custom filenameGenerator function *(optional, default: 'byType')*\n - `defaultFilename`: filename for index page *(optional, default: 'index.html')*\n - `prettifyUrls`: whether urls should be 'prettified', by having the `defaultFilename` removed *(optional, default: false)*\n - `sources`: array of objects to load, specifies selectors and attribute values to select files for loading *(optional, see example below)*\n - `subdirectories`: array of objects, specifies subdirectories for file extensions. If `null` all files will be saved to `directory` *(optional, see example below)*\n - `request`: object, custom options for [request](https://github.com/request/request#requestoptions-callback) *(optional, see example below)*\n - `recursive`: boolean, if `true` scraper will follow anchors in html files. Don't forget to set `maxDepth` to avoid infinite downloading *(optional, see example below)*\n - `maxDepth`: positive number, maximum allowed depth for dependencies *(optional, see example below)*\n \nDefault options you can find in [lib/config/defaults.js](https://github.com/s0ph1e/node-website-scraper/blob/master/lib/config/defaults.js).\n\n\n**callback** - callback function *(optional)*, includes following parameters:\n\n  - `error`: if error - `Error` object, if success - `null`\n  - `result`: if error - `null`, if success - array of [Resource](https://github.com/s0ph1e/node-website-scraper/blob/master/lib/resource.js) objects containing:\n    - `url`: url of loaded page\n    - `filename`: filename where page was saved (relative to `directory`)\n    - `assets`: array of children Resources\n\n### Filename Generators\nThe filename generator determines where the scraped files are saved.\n\n#### byType (default)\nWhen the `byType` filenameGenerator is used the downloaded files are saved by type (as defined by the `subdirectories` setting) \nor directly in the `directory` folder, if no subdirectory is specified for the specific type.\n\n#### bySiteStructure\nWhen the `bySiteStructure` filenameGenerator is used the downloaded files are saved in `directory` using same structure as on the website:\n- `/` => `DIRECTORY/index.html`\n- `/about` => `DIRECTORY/about/index.html`\n- `/resources/javascript/libraries/jquery.min.js` => `DIRECTORY/resources/javascript/libraries/jquery.min.js`\n\n\n## Examples\n#### Example 1\nLet's scrape some pages from [http://nodejs.org/](http://nodejs.org/) with images, css, js files and save them to `/path/to/save/`.\nImagine we want to load:\n  - [Home page](http://nodejs.org/) to `index.html`\n  - [About page](http://nodejs.org/about/) to `about.html`\n  - [Blog](http://blog.nodejs.org/) to `blog.html`\n\nand separate files into directories:\n\n  - `img` for .jpg, .png, .svg (full path `/path/to/save/img`)\n  - `js` for .js (full path `/path/to/save/js`)\n  - `css` for .css (full path `/path/to/save/css`)\n\n```javascript\nvar scraper = require('website-scraper');\nscraper.scrape({\n  urls: [\n    'http://nodejs.org/',\t// Will be saved with default filename 'index.html'\n    {url: 'http://nodejs.org/about', filename: 'about.html'},\n    {url: 'http://blog.nodejs.org/', filename: 'blog.html'}\n  ],\n  directory: '/path/to/save',\n  subdirectories: [\n    {directory: 'img', extensions: ['.jpg', '.png', '.svg']},\n    {directory: 'js', extensions: ['.js']},\n    {directory: 'css', extensions: ['.css']}\n  ],\n  sources: [\n    {selector: 'img', attr: 'src'},\n    {selector: 'link[rel=\"stylesheet\"]', attr: 'href'},\n    {selector: 'script', attr: 'src'}\n  ],\n  request: {\n    headers: {\n      'User-Agent': 'Mozilla/5.0 (Linux; Android 4.2.1; en-us; Nexus 4 Build/JOP40D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Mobile Safari/535.19'\n    }\n  }\n}).then(function (result) {\n  console.log(result);\n}).catch(function(err){\n  console.log(err);\n});\n```\n\n#### Example 2. Recursive downloading\n```javascript\n// Links from example.com will be followed\n// Links from links will be ignored because theirs depth = 2 is greater than maxDepth\nvar scraper = require('website-scraper');\nscraper.scrape({\n  urls: ['http://example.com/'],\n  directory: '/path/to/save',\n  recursive: true,\n  maxDepth: 1\n}).then(console.log).catch(console.log);\n```\n\n#### Example 3. Filtering out external resources\n```javascript\n// Links to other websites are filtered out by the urlFilter\nvar scraper = require('website-scraper');\nscraper.scrape({\n  urls: ['http://example.com/'],\n  urlFilter: function(url){\n    return url.indexOf('http://example.com') === 0;\n  },\n  directory: '/path/to/save'\n}).then(console.log).catch(console.log);\n```\n\n#### Example 4. Downloading an entire website\n```javascript\n// Downloads all the crawlable files of example.com.\n// The files are saved in the same structure as the structure of the website, by using the `bySiteStructure` filenameGenerator.\n// Links to other websites are filtered out by the urlFilter\nvar scraper = require('website-scraper');\nscraper.scrape({\n  urls: ['http://example.com/'],\n  urlFilter: function(url){\n      return url.indexOf('http://example.com') === 0;\n  },\n  recursive: true,\n  maxDepth: 100,\n  prettifyUrls: true,\n  filenameGenerator: 'bySiteStructure',\n  directory: '/path/to/save'\n}).then(console.log).catch(console.log);\n```\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git://github.com/dlfinis/sigma-node-website-scraper.git"
  },
  "scripts": {
    "eslint": "eslint lib/** index.js",
    "test": "istanbul cover _mocha --dir ./coverage --report lcov -- -R spec --recursive --timeout 7000 ./test && npm run eslint"
  },
  "version": "1.0.4"
}
